{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Project 3: Collaboration and Competition\n",
    "### Udacity Deep Reinforcement Learning Nanodegree\n",
    "by Kitson Swann\n",
    "2021-05-14\n",
    "\n",
    "## Tennis Environment\n",
    "\n",
    "![playing.gif](playing.gif)\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "### States\n",
    "\n",
    "The state space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. \n",
    "\n",
    "### Actions\n",
    "\n",
    "Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "The task is episodic. The task is considered solved when the maximum of the total score for the two agents is +0.5 (over 100 consecutive episodes).\n",
    "\n",
    "After each episode, we add up the rewards that each agent received (without discounting, to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "This yields a single score for each episode.\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the environment initialized with the state and action spaces shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.29742813 -1.5\n",
      " -0.          0.          7.17024279  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "#from optimize import find_optimal_hyperparameters\n",
    "from train import train_ddpg\n",
    "\n",
    "RANDOM_SEED = 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "env = UnityEnvironment(file_name='./env/Tennis.app', seed=RANDOM_SEED, no_graphics=True)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "I used the Deep Deterministic Policy Gradient agent I had previously developed for Project 2, which was originally based on the Pendulum environment example [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum). I modified the training loop to work with the Unity Tennis environment, which required some minor changes from the Project 2 version. In particular the calculation of the rewards is different, and requires taking the episode max over two competing agents.\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm is similar to a Deep Q Network (DQN) but adapted and modified to suit the task of solving environments with continuous action spaces.\n",
    "\n",
    "The DDPG algorithm is similar to a DQN in that:\n",
    "- It uses a replay buffer to store experience tuples which it samples mini-batches from to perform gradient descent steps on the network. \n",
    "- It also makes use of local and a target networks to make target values more stationary while training.\n",
    "\n",
    "However, DQN estimates a Q function that outputs a value for each possible discrete action. Then the agent greedily chooses the action with the highest value. The DDPG instead includes:\n",
    "- an actor - which is a deterministic policy network that takes in a state and returns an action to take directly and,\n",
    "- a critic - a value network which takes in the state plus the action from the actor, and returns the value of the state/action pair\n",
    "\n",
    "In the learning step in DDPG:\n",
    "\n",
    "- A minibatch is sampled from the replay buffer\n",
    "- The target actor network predicts the next actions from the states\n",
    "- The target critic predicts the values of the chosen actions\n",
    "- The Q targets are calculated\n",
    "- The Q expected values are calculated using the local critic\n",
    "- The critic loss is calculated between the expected and target Q values\n",
    "- The loss is minimized on the local critic network\n",
    "- The local actor network predicts the next actions\n",
    "- The actor loss is calculated using the local critic\n",
    "- The loss is minimized on the local actor network\n",
    "- The actor and critic target networks are soft updated with the local network parameters\n",
    "\n",
    "### Initial Experimentation\n",
    "\n",
    "I started with the hyperparameters I had used for Project 2. The Tennis episodes are not static in length like that of the Reacher environment, since the episode continues as long as the ball is in the air. In the early stages the episodes end quickly and they get longer in duration as the agents progress in skill and can keep the ball in the air. I initially ran a training run of 2000 episodes, and saw promising progress on scores, so I increased the training to 20,000 episodes. To my suprise, the agent solved the task in 4215 episodes.\n",
    "\n",
    "I am not totally suprised by this result, as hyper-parameter choices in Deep Learning often end up being good rules of thumb, and work across many similar tasks. This is one the principles they teach in the Fast.ai course for getting good results in training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"random_seed\": 2,\n",
      "  \"noise_sigma\": 0.02,\n",
      "  \"noise_theta\": 0.015,\n",
      "  \"weight_decay\": 0,\n",
      "  \"tau\": 0.05,\n",
      "  \"gamma\": 0.95,\n",
      "  \"critic_lr\": 0.0001,\n",
      "  \"actor_lr\": 0.001,\n",
      "  \"critic_fc2_units\": 128,\n",
      "  \"critic_fc1_units\": 128,\n",
      "  \"actor_fc2_units\": 128,\n",
      "  \"actor_fc1_units\": 128,\n",
      "  \"update_times\": 10,\n",
      "  \"update_every\": 20,\n",
      "  \"batch_size\": 256,\n",
      "  \"buffer_size\": 1000000.0,\n",
      "  \"eps_decay\": 0.7,\n",
      "  \"eps_end\": 0.3,\n",
      "  \"eps_start\": 1.0,\n",
      "  \"action_size\": 2,\n",
      "  \"state_size\": 24,\n",
      "  \"n_episodes\": 20000,\n",
      "  \"solved_threshold\": 0.5,\n",
      "  \"break_early\": true,\n",
      "  \"name\": \"_base\",\n",
      "  \"num_agents\": 2,\n",
      "  \"brain_name\": \"TennisBrain\"\n",
      "Replay Buffer Size: 1000000.0, Batch Size: 256\n",
      "Episode 9\tAverage Score: 0.0\tScore: 0.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitson_swann/Documents/udacity_drl/udacity_drl_p3/agent.py:166: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 200\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 300\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 400\tAverage Score: 0.0546\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 500\tAverage Score: 0.0653\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 600\tAverage Score: 0.0764\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 700\tAverage Score: 0.0646\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 800\tAverage Score: 0.0645\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 900\tAverage Score: 0.079\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1000\tAverage Score: 0.0811\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1100\tAverage Score: 0.0837\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1200\tAverage Score: 0.0844\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1300\tAverage Score: 0.0851\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1400\tAverage Score: 0.0888\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 1500\tAverage Score: 0.0855\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1600\tAverage Score: 0.0868\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1700\tAverage Score: 0.0793\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 1800\tAverage Score: 0.0916\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 1900\tAverage Score: 0.0805\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 2000\tAverage Score: 0.0796\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2100\tAverage Score: 0.0851\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2200\tAverage Score: 0.0866\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2300\tAverage Score: 0.0858\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2400\tAverage Score: 0.08\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2500\tAverage Score: 0.075\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2600\tAverage Score: 0.0914\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2700\tAverage Score: 0.103\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2800\tAverage Score: 0.0874\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2900\tAverage Score: 0.0844\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3000\tAverage Score: 0.0926\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 3100\tAverage Score: 0.0849\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3200\tAverage Score: 0.0817\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3300\tAverage Score: 0.0962\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3400\tAverage Score: 0.1032\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 3500\tAverage Score: 0.0912\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3600\tAverage Score: 0.1006\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3700\tAverage Score: 0.1272\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3800\tAverage Score: 0.1528\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 3900\tAverage Score: 0.2493\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 4000\tAverage Score: 0.1985\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 4100\tAverage Score: 0.3265\tMax Score: 0.6\tMin Score: 0.6\n",
      "Episode 4200\tAverage Score: 0.4478\tMax Score: 1.0\tMin Score: 1.0\n",
      "Episode 4215\tAverage Score: 0.5047\tScore: 0.79\n",
      "Environment solved in 4215 episodes!\tAverage Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "optimal_params = {\n",
    "  \"random_seed\": RANDOM_SEED,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 128,\n",
    "  \"critic_fc1_units\": 128,\n",
    "  \"actor_fc2_units\": 128,\n",
    "  \"actor_fc1_units\": 128,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": action_size,\n",
    "  \"state_size\": state_size,\n",
    "  \"n_episodes\": 20000,\n",
    "  \"solved_threshold\": 0.5,\n",
    "  \"break_early\": True,\n",
    "  \"name\": \"_base\",\n",
    "  \"num_agents\": 2,\n",
    "  \"brain_name\": brain_name\n",
    "}\n",
    "\n",
    "scores = train_ddpg(env=env, **optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"scores.json\", \"w\") as f:\n",
    "    json.dump({\"scores\": scores}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Average Scores Over 1000 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd52e7fada0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"scores.json\", \"r\") as f:\n",
    "    scores = json.load(f)\n",
    "\n",
    "sdf = pd.DataFrame(scores)\n",
    "sdf = sdf.reset_index().rename(columns={\"index\": \"episode\", \"scores\": \"max_agent_score\"})\n",
    "sdf[\"mean_of_max_agent_score\"] = sdf[\"max_agent_score\"].rolling(window=100).mean()\n",
    "\n",
    "sns.lineplot(data=sdf, x=\"episode\", y=\"mean_of_max_agent_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch The Trained Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've loaded the trained agent and shown a gif of it performing the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer Size: 1000000.0, Batch Size: 256\n",
      "Episode: 0\tMax Reward: 0.19\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: 0.19\n",
      "Episode: 1\tMax Reward: 2.6\tAgent 1 Total Reward: 2.6\tAgent 2 Total Reward: 2.6\n",
      "Timestep: 24\tAgent 1 Reward: 0.0\tAgent 2 Reward: 0.0"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='./env/Tennis.app')\n",
    "\n",
    "RANDOM_SEED=2\n",
    "NUM_AGENTS=2\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "from agent import Agent\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "  \"random_seed\": RANDOM_SEED,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 128,\n",
    "  \"critic_fc1_units\": 128,\n",
    "  \"actor_fc2_units\": 128,\n",
    "  \"actor_fc1_units\": 128,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": 2,\n",
    "  \"state_size\": 24,\n",
    "}\n",
    "\n",
    "agent = Agent(**params)\n",
    "\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor_base.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic_base.pth'))\n",
    "        \n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "max_scores = []\n",
    "\n",
    "for i_episode in range(10):\n",
    "    agent.reset()\n",
    "    states = env_info.vector_observations\n",
    "    agent_scores = np.zeros(NUM_AGENTS)\n",
    "    i = 1\n",
    "    while True:\n",
    "        actions = agent.act(state=states, add_noise=False)\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        states = next_states\n",
    "        agent_scores += rewards\n",
    "        print(f\"\\rTimestep: {i}\\tAgent 1 Reward: {round(rewards[0],4)}\\tAgent 2 Reward: {round(rewards[1],4)}\", end = \"\")\n",
    "        if np.any(dones):\n",
    "            break\n",
    "        i += 1\n",
    "    \n",
    "    episode_max = np.max(agent_scores)\n",
    "    print(f\"\\rEpisode: {i_episode}\\tMax Reward: {round(episode_max,4)}\\tAgent 1 Total Reward: {round(agent_scores[0],4)}\\tAgent 2 Total Reward: {round(agent_scores[1],4)}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![playing.gif](playing.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run with Larger Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above training session the agent solves the environment, but the play didn't look very natural. There is a lot of back and fourth movement that seems uneccessary that a human agent wouldn't do. I was curious to see if a larger network would change this. The larger network took much longer to get to a point where the agents were able to rally, and it came close to achieving the 0.5 threshold but then appeared to get worse over time, so I terminated it early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"random_seed\": 2,\n",
      "  \"noise_sigma\": 0.02,\n",
      "  \"noise_theta\": 0.015,\n",
      "  \"weight_decay\": 0,\n",
      "  \"tau\": 0.05,\n",
      "  \"gamma\": 0.95,\n",
      "  \"critic_lr\": 0.0001,\n",
      "  \"actor_lr\": 0.001,\n",
      "  \"critic_fc2_units\": 300,\n",
      "  \"critic_fc1_units\": 400,\n",
      "  \"actor_fc2_units\": 300,\n",
      "  \"actor_fc1_units\": 400,\n",
      "  \"update_times\": 10,\n",
      "  \"update_every\": 20,\n",
      "  \"batch_size\": 256,\n",
      "  \"buffer_size\": 1000000.0,\n",
      "  \"eps_decay\": 0.7,\n",
      "  \"eps_end\": 0.3,\n",
      "  \"eps_start\": 1.0,\n",
      "  \"action_size\": 2,\n",
      "  \"state_size\": 24,\n",
      "  \"n_episodes\": 20000,\n",
      "  \"solved_threshold\": 0.5,\n",
      "  \"break_early\": true,\n",
      "  \"name\": \"_large\",\n",
      "  \"num_agents\": 2,\n",
      "  \"brain_name\": \"TennisBrain\"\n",
      "Replay Buffer Size: 1000000.0, Batch Size: 256\n",
      "Episode 9\tAverage Score: 0.0\tScore: 0.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitson_swann/Documents/udacity_drl/udacity_drl_p3/agent.py:166: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 200\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 300\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 400\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 500\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 600\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 700\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 800\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 900\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1000\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1100\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1200\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1300\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1400\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1500\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1600\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1700\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1800\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1900\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 2000\tAverage Score: 0.0076\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 2100\tAverage Score: 0.0175\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 2200\tAverage Score: 0.0445\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2300\tAverage Score: 0.1028\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2400\tAverage Score: 0.1001\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2500\tAverage Score: 0.1107\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2600\tAverage Score: 0.106\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2700\tAverage Score: 0.1642\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2800\tAverage Score: 0.3228\tMax Score: 0.7\tMin Score: 0.7\n",
      "Episode 2900\tAverage Score: 0.4343\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3000\tAverage Score: 0.4205\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3100\tAverage Score: 0.2977\tMax Score: 0.3\tMin Score: 0.3\n",
      "Episode 3200\tAverage Score: 0.2408\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 3300\tAverage Score: 0.1807\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3400\tAverage Score: 0.1958\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 3500\tAverage Score: 0.2045\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 3600\tAverage Score: 0.2004\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3700\tAverage Score: 0.1805\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 3800\tAverage Score: 0.1729\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 3900\tAverage Score: 0.1718\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 4000\tAverage Score: 0.1757\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 4100\tAverage Score: 0.1867\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 4200\tAverage Score: 0.1836\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 4300\tAverage Score: 0.1769\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 4400\tAverage Score: 0.169\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 4500\tAverage Score: 0.1729\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 4600\tAverage Score: 0.1584\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 4700\tAverage Score: 0.147\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 4800\tAverage Score: 0.16\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 4900\tAverage Score: 0.1597\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 5000\tAverage Score: 0.1629\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 5100\tAverage Score: 0.1515\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 5200\tAverage Score: 0.1479\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 5300\tAverage Score: 0.1449\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 5400\tAverage Score: 0.1368\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 5500\tAverage Score: 0.136\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 5600\tAverage Score: 0.1438\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 5700\tAverage Score: 0.1379\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 5783\tAverage Score: 0.1448\tScore: 0.19"
     ]
    }
   ],
   "source": [
    "alternate_params = {\n",
    "  \"random_seed\": RANDOM_SEED,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 300,\n",
    "  \"critic_fc1_units\": 400,\n",
    "  \"actor_fc2_units\": 300,\n",
    "  \"actor_fc1_units\": 400,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": action_size,\n",
    "  \"state_size\": state_size,\n",
    "  \"n_episodes\": 20000,\n",
    "  \"solved_threshold\": 0.5,\n",
    "  \"break_early\": True,\n",
    "  \"name\": \"_large\",\n",
    "  \"num_agents\": 2,\n",
    "  \"brain_name\": brain_name\n",
    "}\n",
    "\n",
    "scores = train_ddpg(env=env, **alternate_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for Future Work\n",
    "\n",
    "I did not do an extensive hyperparameter search here because I had already solved the environent, and because the agent didn't show any positive scores until 400 episodes into training. So, to get any signal from the hyperparameter optimization I would probably have to train each parameter set for a minimum of 400 episodes, which would take a prohibitively long time. If I had more time and access to a GPU I could conduct a more exhaustive hyper-parameter search.\n",
    "\n",
    "In the previous project instructions, there were also number of other algorithms mentioned that may work better including:\n",
    "\n",
    "- Trust Region Policy Optimization (TRPO)\n",
    "- Truncated Natural Policy Gradient (TNPG)\n",
    "- Proximal Policy Optimization (PPO)\n",
    "- Distributed Distributional Deterministic Policy Gradients (D4PG)\n",
    "\n",
    "The next steps for me would be to investigate and attempt to solve the task with one or more of these alternate algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
