{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Project 3: Collaboration and Competition\n",
    "### Udacity Deep Reinforcement Learning Nanodegree\n",
    "by Kitson Swann\n",
    "\n",
    "## Tennis Environment\n",
    "\n",
    "![playing.gif](playing.gif)\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "### States\n",
    "\n",
    "The state space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. \n",
    "\n",
    "### Actions\n",
    "\n",
    "Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "# Rewards\n",
    "\n",
    "The task is episodic. The task is considered solved when the maximum of the total score for the two agents is +0.5 (over 100 consecutive episodes).\n",
    "\n",
    "After each episode, we add up the rewards that each agent received (without discounting, to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "This yields a single score for each episode.\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Version 1\n",
    "\n",
    "For this project I had planned to initially begin with the simple single agent environment and move onto the more complex multi-agent environment after solving the simple one, but achieving the minimum average score on the single agent version proved quite difficult, so I stopped at solving just the simple environment. Below you can see the agent initialized with the state and action spaces shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.29742813 -1.5\n",
      " -0.          0.          7.17024279  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "#from optimize import find_optimal_hyperparameters\n",
    "from train import train_ddpg\n",
    "\n",
    "RANDOM_SEED = 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "env = UnityEnvironment(file_name='./env/Tennis.app', seed=RANDOM_SEED, no_graphics=True)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.10000000149011612\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "As suggested in the project instructions I used the Deep Deterministic Policy Gradient agent from the Pendulum environment example [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum) as a starting point. I modified the training loop to work with the Unity Reacher environment, which required some minor changes from the original which was based on an OpenAI Gym environment.\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm is similar to a Deep Q Network (DQN) but adapted and modified to suit the task of solving environments with continuous action spaces.\n",
    "\n",
    "The DDPG algorithm is similar to a DQN in that:\n",
    "- It uses a replay buffer to store experience tuples which it samples mini-batches from to perform gradient descent steps on the network. \n",
    "- It also makes use of local and a target networks to make target values more stationary while training.\n",
    "\n",
    "However, DQN estimates a Q function that outputs a value for each possible discrete action. Then the agent greedily chooses the action with the highest value. The DDPG instead includes:\n",
    "- an actor - which is a deterministic policy network that takes in a state and returns an action to take directly and,\n",
    "- a critic - a value network which takes in the state plus the action from the actor, and returns the value of the state/action pair\n",
    "\n",
    "In the learning step in DDPG:\n",
    "\n",
    "- A minibatch is sampled from the replay buffer\n",
    "- The target actor network predicts the next actions from the states\n",
    "- The target critic predicts the values of the chosen actions\n",
    "- The Q targets are calculated\n",
    "- The Q expected values are calculated using the local critic\n",
    "- The critic loss is calculated between the expected and target Q values\n",
    "- The loss is minimized on the local critic network\n",
    "- The local actor network predicts the next actions\n",
    "- The actor loss is calculated using the local critic\n",
    "- The loss is minimized on the local actor network\n",
    "- The actor and critic target networks are soft updated with the local network parameters\n",
    "\n",
    "### Initial Experimentation\n",
    "\n",
    "After getting the training routine working, I initially had no success in getting the agent to train with the standard hyper-parameters. The score was oscillating around 0-0.1 most of the time and not improving even over 100 episodes.\n",
    "\n",
    "Normally at this point I would have experimented with hyperparameter-optimization, but in this algorithm there are so many parameters that I felt the need to look for some good values. I consulted Udacity's Knowledge Base for some assistance, and saw a number of suggestions. I reviewed the suggestions and many repositories containing the solutions of other students for ideas. I made a number of changes as a result including:\n",
    "\n",
    "- Changing the training loop to end when done which for the Reacher environment is 1000 timesteps.\n",
    "- Trying different random seeds.\n",
    "- Modified the architecture of the actor and critic neural networks to try different hidden layer sizes.\n",
    "- Modified the architecture of the actor and critic neural networks to add batch normalization.\n",
    "- Modified the DDPG agent to only do a learning step every 5, 10, 20, 40 iterations.\n",
    "- Reset the noise after every episode.\n",
    "- Tried different suggested values for buffer size, batch size, gamma, tau, weight decay and the sigma variable in the Ornstein-Uhlenbeck process noise implementation.\n",
    "\n",
    "Even with all of the above modifications I still wasn't seeing any decent results. The score rarely went above 0.5, and seemed to get worse over time.\n",
    "\n",
    "### Structured Hyper-parameter Optimization\n",
    "\n",
    "After seeing a number of people say the suggested changes should work, but having no success myself, I felt that I needed to take a more robust approach to hyper-parameter optimization. Similar to my implementation in Project 1, I used scikit-optimize's Gaussian Process Optimizer to search for a better set of hyper-parameters. Below is the search space that I initially defined.\n",
    "\n",
    "I initially did batches of 10 episodes to keep the run time short and look for some better initial values. Because the process takes a long time I watched the values from each run, and visually tried to understand what values of each parameter were working well. As I noticed good values, I iteratively removed parameters from the search space, instead setting them at reasonable levels until I found something that worked fairly well which I've called Reasonable Parameter Set below.\n",
    "\n",
    "I have included the call to the optimization routine below, but haven't run it as it takes up a lot of space, and I re-ran it iteratively so one single output was never the final one used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "space = [\n",
    "    Real(0, 0.5, \"uniform\", name='eps_end'),\n",
    "    Real(1e-5, 1e0, \"log-uniform\", name='eps_decay'),\n",
    "    Categorical([1e5, 1e6], name=\"buffer_size\"),\n",
    "    Categorical([64, 128, 256, 512], name=\"batch_size\"),\n",
    "    Categorical([1, 5, 10, 20, 40], name=\"update_every\"),\n",
    "    Categorical([1, 5, 10, 20], name=\"update_times\"),\n",
    "    Categorical([128, 256, 512], name=\"actor_fc1_units\"),\n",
    "    Categorical([128, 256, 512], name=\"actor_fc2_units\"),\n",
    "    Categorical([128, 256, 512], name=\"critic_fc1_units\"),\n",
    "    Categorical([128, 256, 512], name=\"critic_fc2_units\"),\n",
    "    Real(1e-5, 1e-3, \"log-uniform\", name='actor_lr'),\n",
    "    Real(1e-5, 1e-3, \"log-uniform\", name='critic_lr'),\n",
    "    Real(0.9, 0.99, \"uniform\", name=\"gamma\"),\n",
    "    Real(1e-2, 1e-1, \"log-uniform\", name=\"tau\"),\n",
    "    Real(0, 1e-3, \"uniform\", name=\"weight_decay\"),\n",
    "    Real(0.01, 0.05, \"log-uniform\", name=\"noise_theta\"),\n",
    "    Real(0.01, 0.05, \"log-uniform\", name=\"noise_sigma\"),\n",
    "]\n",
    "\n",
    "params, res_gp = find_optimal_hyperparameters(env=env, brain_name=brain_name, num_agents=num_agents, n_calls=50, episodes_per_batch=10, space=space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasonable Parameter Set\n",
    "\n",
    "This is the best resonable parameter set I found through a somewhat manual search for hyper-parameters. There are many parameters to tune here, and the search space grows so quickly that a full optimization run was going to take more time than I had available, so I viewed the results and manually tuned based on what I saw to develop the set of values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"random_seed\": 2,\n",
      "  \"noise_sigma\": 0.02,\n",
      "  \"noise_theta\": 0.015,\n",
      "  \"weight_decay\": 0,\n",
      "  \"tau\": 0.05,\n",
      "  \"gamma\": 0.95,\n",
      "  \"critic_lr\": 0.0001,\n",
      "  \"actor_lr\": 0.001,\n",
      "  \"critic_fc2_units\": 128,\n",
      "  \"critic_fc1_units\": 128,\n",
      "  \"actor_fc2_units\": 128,\n",
      "  \"actor_fc1_units\": 128,\n",
      "  \"update_times\": 10,\n",
      "  \"update_every\": 20,\n",
      "  \"batch_size\": 256,\n",
      "  \"buffer_size\": 1000000.0,\n",
      "  \"eps_decay\": 0.7,\n",
      "  \"eps_end\": 0.3,\n",
      "  \"eps_start\": 1.0,\n",
      "  \"action_size\": 2,\n",
      "  \"state_size\": 24,\n",
      "  \"n_episodes\": 20000,\n",
      "  \"solved_threshold\": 0.5,\n",
      "  \"break_early\": true,\n",
      "  \"name\": \"_base\",\n",
      "  \"num_agents\": 2,\n",
      "  \"brain_name\": \"TennisBrain\"\n",
      "Replay Buffer Size: 1000000.0, Batch Size: 256\n",
      "Episode 9\tAverage Score: 0.0\tScore: 0.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitson_swann/Documents/udacity_drl/udacity_drl_p3/agent.py:166: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 200\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 300\tAverage Score: 0.0\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 400\tAverage Score: 0.0546\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 500\tAverage Score: 0.0653\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 600\tAverage Score: 0.0764\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 700\tAverage Score: 0.0646\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 800\tAverage Score: 0.0645\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 900\tAverage Score: 0.079\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1000\tAverage Score: 0.0811\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1100\tAverage Score: 0.0837\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1200\tAverage Score: 0.0844\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1300\tAverage Score: 0.0851\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1400\tAverage Score: 0.0888\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 1500\tAverage Score: 0.0855\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 1600\tAverage Score: 0.0868\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 1700\tAverage Score: 0.0793\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 1800\tAverage Score: 0.0916\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 1900\tAverage Score: 0.0805\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 2000\tAverage Score: 0.0796\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2100\tAverage Score: 0.0851\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2200\tAverage Score: 0.0866\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2300\tAverage Score: 0.0858\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2400\tAverage Score: 0.08\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2500\tAverage Score: 0.075\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2600\tAverage Score: 0.0914\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 2700\tAverage Score: 0.103\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2800\tAverage Score: 0.0874\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 2900\tAverage Score: 0.0844\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3000\tAverage Score: 0.0926\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 3100\tAverage Score: 0.0849\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3200\tAverage Score: 0.0817\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3300\tAverage Score: 0.0962\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3400\tAverage Score: 0.1032\tMax Score: 0.09\tMin Score: 0.09\n",
      "Episode 3500\tAverage Score: 0.0912\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3600\tAverage Score: 0.1006\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3700\tAverage Score: 0.1272\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 3800\tAverage Score: 0.1528\tMax Score: 0.0\tMin Score: 0.0\n",
      "Episode 3900\tAverage Score: 0.2493\tMax Score: 0.1\tMin Score: 0.1\n",
      "Episode 4000\tAverage Score: 0.1985\tMax Score: 0.2\tMin Score: 0.2\n",
      "Episode 4100\tAverage Score: 0.3265\tMax Score: 0.6\tMin Score: 0.6\n",
      "Episode 4195\tAverage Score: 0.4288\tScore: 0.69"
     ]
    }
   ],
   "source": [
    "optimal_params = {\n",
    "  \"random_seed\": RANDOM_SEED,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 128,\n",
    "  \"critic_fc1_units\": 128,\n",
    "  \"actor_fc2_units\": 128,\n",
    "  \"actor_fc1_units\": 128,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": action_size,\n",
    "  \"state_size\": state_size,\n",
    "  \"n_episodes\": 20000,\n",
    "  \"solved_threshold\": 0.5,\n",
    "  \"break_early\": True,\n",
    "  \"name\": \"_base\",\n",
    "  \"num_agents\": 2,\n",
    "  \"brain_name\": brain_name\n",
    "}\n",
    "\n",
    "scores = train_ddpg(env=env, **optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"scores.json\", \"w\") as f:\n",
    "    json.dump({\"scores\": scores}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success !!!\n",
    "\n",
    "As you can see above the environment was solved in 639 episodes. In reality, it was a bit shorter than that, because I set the threshold to 31 just to go a bit beyond the minimim value.\n",
    "\n",
    "Below you can see the plots of raw scores and average scores that I took from Tensorboard.\n",
    "\n",
    "![tensorboard_raw_score.png](tensorboard_raw_score.png)\n",
    "![tensorboard_avg_score.png](tensorboard_avg_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch The Trained Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've loaded the trained agent and shown a gif of it performing the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer Size: 1000000.0, Batch Size: 256\n",
      "Episode: 0\tMax Reward: 0.09\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: 0.09\n",
      "Episode: 1\tMax Reward: 0.09\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: 0.09\n",
      "Episode: 2\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: -0.01\n",
      "Episode: 3\tMax Reward: 0.0\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: -0.01\n",
      "Episode: 4\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: -0.01\n",
      "Episode: 5\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: -0.01\n",
      "Episode: 6\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: 0.09\n",
      "Episode: 7\tMax Reward: 0.0\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: -0.01\n",
      "Episode: 8\tMax Reward: 0.0\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: -0.01\n",
      "Episode: 9\tMax Reward: 0.0\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: -0.01\n",
      "Episode: 10\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: -0.01\n",
      "Episode: 11\tMax Reward: 0.09\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: 0.09\n",
      "Episode: 12\tMax Reward: 0.0\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: -0.01\n",
      "Episode: 13\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: -0.01\n",
      "Episode: 14\tMax Reward: 0.0\tAgent 1 Total Reward: -0.01\tAgent 2 Total Reward: 0.0\n",
      "Episode: 15\tMax Reward: 0.09\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: 0.09\n",
      "Episode: 16\tMax Reward: 0.1\tAgent 1 Total Reward: -0.01\tAgent 2 Total Reward: 0.1\n",
      "Episode: 17\tMax Reward: 0.0\tAgent 1 Total Reward: -0.01\tAgent 2 Total Reward: 0.0\n",
      "Episode: 18\tMax Reward: 0.09\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: 0.09\n",
      "Episode: 19\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: -0.01\n",
      "Episode: 20\tMax Reward: 0.1\tAgent 1 Total Reward: -0.01\tAgent 2 Total Reward: 0.1\n",
      "Episode: 21\tMax Reward: 0.09\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: 0.09\n",
      "Episode: 22\tMax Reward: 0.0\tAgent 1 Total Reward: -0.01\tAgent 2 Total Reward: 0.0\n",
      "Episode: 23\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: -0.01\n",
      "Episode: 24\tMax Reward: 0.1\tAgent 1 Total Reward: 0.1\tAgent 2 Total Reward: -0.01\n",
      "Episode: 25\tMax Reward: 0.0\tAgent 1 Total Reward: 0.0\tAgent 2 Total Reward: -0.01\n",
      "Episode: 26\tMax Reward: 0.0\tAgent 1 Total Reward: -0.01\tAgent 2 Total Reward: 0.0\n",
      "Episode: 27\tMax Reward: 0.0\tAgent 1 Total Reward: -0.01\tAgent 2 Total Reward: 0.0\n",
      "Timestep: 10\tAgent 1 Reward: 0.0\tAgent 2 Reward: 0.0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-1a5a2f722d3f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     52\u001B[0m         \u001B[0mactions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstates\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madd_noise\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m         \u001B[0mactions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m         \u001B[0menv_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbrain_name\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     55\u001B[0m         \u001B[0mnext_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv_info\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvector_observations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m         \u001B[0mrewards\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv_info\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrewards\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/udacity_drl_p3/lib/python3.6/site-packages/unityagents/environment.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, vector_action, memory, text_action)\u001B[0m\n\u001B[1;32m    367\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    368\u001B[0m             outputs = self.communicator.exchange(\n\u001B[0;32m--> 369\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_generate_step_input\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvector_action\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmemory\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext_action\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    370\u001B[0m             )\n\u001B[1;32m    371\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0moutputs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/udacity_drl_p3/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001B[0m in \u001B[0;36mexchange\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     76\u001B[0m         \u001B[0mmessage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munity_input\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCopyFrom\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munity_to_external\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparent_conn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 78\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munity_to_external\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparent_conn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     79\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m200\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/udacity_drl_p3/lib/python3.6/multiprocessing/connection.py\u001B[0m in \u001B[0;36mrecv\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    248\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_closed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    249\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_readable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 250\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    251\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_ForkingPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetbuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/udacity_drl_p3/lib/python3.6/multiprocessing/connection.py\u001B[0m in \u001B[0;36m_recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    406\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 407\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    408\u001B[0m         \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"!i\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmaxsize\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0msize\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/udacity_drl_p3/lib/python3.6/multiprocessing/connection.py\u001B[0m in \u001B[0;36m_recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    377\u001B[0m         \u001B[0mremaining\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mremaining\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 379\u001B[0;31m             \u001B[0mchunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mremaining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    380\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='./env/Tennis.app')\n",
    "\n",
    "RANDOM_SEED=2\n",
    "NUM_AGENTS=2\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "from agent import Agent\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "  \"random_seed\": RANDOM_SEED,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 128,\n",
    "  \"critic_fc1_units\": 128,\n",
    "  \"actor_fc2_units\": 128,\n",
    "  \"actor_fc1_units\": 128,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": 2,\n",
    "  \"state_size\": 24,\n",
    "}\n",
    "\n",
    "agent = Agent(**params)\n",
    "\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor_base.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic_base.pth'))\n",
    "        \n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "max_scores = []\n",
    "\n",
    "for i_episode in range(1000):\n",
    "    agent.reset()\n",
    "    states = env_info.vector_observations\n",
    "    agent_scores = np.zeros(NUM_AGENTS)\n",
    "    i = 1\n",
    "    while True:\n",
    "        actions = agent.act(state=states, add_noise=False)\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        states = next_states\n",
    "        agent_scores += rewards\n",
    "        print(f\"\\rTimestep: {i}\\tAgent 1 Reward: {round(rewards[0],4)}\\tAgent 2 Reward: {round(rewards[1],4)}\", end = \"\")\n",
    "        if np.any(dones):\n",
    "            break\n",
    "        i += 1\n",
    "    \n",
    "    episode_max = np.max(agent_scores)\n",
    "    print(f\"\\rEpisode: {i_episode}\\tMax Reward: {round(episode_max,4)}\\tAgent 1 Total Reward: {round(agent_scores[0],4)}\\tAgent 2 Total Reward: {round(agent_scores[1],4)}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![playing.gif](playing.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Training Run with Larger Network\n",
    "\n",
    "I wanted to see how much better the score could get with a slightly longer training run and networks with larger hidden layers. I also modified the batch size since it seems to have an effect on how long each iteration takes.\n",
    "\n",
    "Below you can see that performance improved slightly breaking the previous score, but it seemed to be hitting a plateau just above 30, so I terminated it.\n",
    "\n",
    "After understanding that there are always 1000 time steps in an episode, and the rewards are closer to 0.04 rather than 0.10 as per timestep that the arm is touching the goal, I realized there is an upper limit on the score of 0.40 * 1000 = 40. I think this is why the agent plateues just after a score of 30, because it is getting closer and closer to a perfect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternate_params = {\n",
    "  \"random_seed\": 22,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 300,\n",
    "  \"critic_fc1_units\": 400,\n",
    "  \"actor_fc2_units\": 300,\n",
    "  \"actor_fc1_units\": 400,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 500000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": 4,\n",
    "  \"state_size\": 33,\n",
    "  \"n_episodes\": 5000,\n",
    "  \"solved_threshold\": 31.0,\n",
    "  \"break_early\": False,\n",
    "  \"name\": \"_alternate\",\n",
    "  \"num_agents\": 1,\n",
    "  \"brain_name\": \"ReacherBrain\"\n",
    "}\n",
    "\n",
    "scores = train_ddpg(env=env, **alternate_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for Future Work\n",
    "\n",
    "If I had more time and access to a GPU I could conduct a more exhaustive hyper-parameter search, letting the agent run for at least 100 episodes each time and I would probably find a better set of parameters, but I don't think this would lead to a significantly better solution for the reason I mentioned above that there isan upper limit on performance. It may lead to a faster to train and more efficient solution though.\n",
    "\n",
    "Using the same algorithm, I could also make slight modifications to train with the multi-agent version of the Reacher environment. I did try this but didn't see initially better performance and it seemed that the hyper-parameters were more important. Now that I've found some decent options I could further investigate the multi-agent version.\n",
    "\n",
    "In the project instructions, there were also number of other algorithms mentioned that may work better including:\n",
    "\n",
    "- Trust Region Policy Optimization (TRPO)\n",
    "- Truncated Natural Policy Gradient (TNPG)\n",
    "- Proximal Policy Optimization (PPO)\n",
    "- Distributed Distributional Deterministic Policy Gradients (D4PG)\n",
    "\n",
    "The next steps for me would be to investigate and attempt to solve the task with one or more of these alternate algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}