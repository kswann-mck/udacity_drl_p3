{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Project 2 - Continuous Control\n",
    "### Udacity Deep Reinforcement Learning Nanodegree\n",
    "by Kitson Swann\n",
    "\n",
    "## Reacher Environment\n",
    "\n",
    "In this environment, our goal is to use a Deep Reinforcement Learning Agent to train a double-jointed arm to \n",
    "move its hand to a target location.\n",
    "\n",
    "For the purpose of this assignment, the task is considered solved when the agent achieves a score of +30 \n",
    "over 100 consecutive episodes.\n",
    "\n",
    "### States\n",
    "\n",
    "The state space contains 33 dimensions corresponding to: \n",
    "\n",
    "- position rotation  \n",
    "- velocity\n",
    "- angular velocities of the arm \n",
    "\n",
    "### Actions\n",
    "\n",
    "Each action is a vector with four numbers, corresponding to torque \n",
    "applicable to two joints. Every entry in the action vector should \n",
    "be a number between -1 and 1. \n",
    "\n",
    "### Rewards\n",
    "\n",
    "According to the project instructions a reward of +0.1 is provided for each step that the agent's hand is in the goal location. However, in practice the rewards are variable and never seem to go above +0.039. I am curious if the environment was updated between the time this assignment was authored and now. The different reward structure did not prevent me from succeeding at the task, but I wonder if it made it more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Version 1\n",
    "\n",
    "For this project I had planned to initially begin with the simple single agent environment and move onto the more complex multi-agent environment after solving the simple one, but achieving the minimum average score on the single agent version proved quite difficult, so I stopped at solving just the simple environment. Below you can see the agent initialized with the state and action spaces shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "from optimize import find_optimal_hyperparameters\n",
    "from train import train_ddpg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "env = UnityEnvironment(file_name='./env/v1/Reacher.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "As suggested in the project instructions I used the Deep Deterministic Policy Gradient agent from the Pendulum environment example [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum) as a starting point. I modified the training loop to work with the Unity Reacher environment, which required some minor changes from the original which was based on an OpenAI Gym environment.\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm is similar to a Deep Q Network (DQN) but adapted and modified to suit the task of solving environments with continuous action spaces.\n",
    "\n",
    "The DDPG algorithm is similar to a DQN in that:\n",
    "- It uses a replay buffer to store experience tuples which it samples mini-batches from to perform gradient descent steps on the network. \n",
    "- It also makes use of local and a target networks to make target values more stationary while training.\n",
    "\n",
    "However, DQN estimates a Q function that outputs a value for each possible discrete action. Then the agent greedily chooses the action with the highest value. The DDPG instead includes:\n",
    "- an actor - which is a deterministic policy network that takes in a state and returns an action to take directly and,\n",
    "- a critic - a value network which takes in the state plus the action from the actor, and returns the value of the state/action pair\n",
    "\n",
    "In the learning step in DDPG:\n",
    "\n",
    "- A minibatch is sampled from the replay buffer\n",
    "- The target actor network predicts the next actions from the states\n",
    "- The target critic predicts the values of the chosen actions\n",
    "- The Q targets are calculated\n",
    "- The Q expected values are calculated using the local critic\n",
    "- The critic loss is calculated between the expected and target Q values\n",
    "- The loss is minimized on the local critic network\n",
    "- The local actor network predicts the next actions\n",
    "- The actor loss is calculated using the local critic\n",
    "- The loss is minimized on the local actor network\n",
    "- The actor and critic target networks are soft updated with the local network parameters\n",
    "\n",
    "### Initial Experimentation\n",
    "\n",
    "After getting the training routine working, I initially had no success in getting the agent to train with the standard hyper-parameters. The score was oscillating around 0-0.1 most of the time and not improving even over 100 episodes.\n",
    "\n",
    "Normally at this point I would have experimented with hyperparameter-optimization, but in this algorithm there are so many parameters that I felt the need to look for some good values. I consulted Udacity's Knowledge Base for some assistance, and saw a number of suggestions. I reviewed the suggestions and many repositories containing the solutions of other students for ideas. I made a number of changes as a result including:\n",
    "\n",
    "- Changing the training loop to end when done which for the Reacher environment is 1000 timesteps.\n",
    "- Trying different random seeds.\n",
    "- Modified the architecture of the actor and critic neural networks to try different hidden layer sizes.\n",
    "- Modified the architecture of the actor and critic neural networks to add batch normalization.\n",
    "- Modified the DDPG agent to only do a learning step every 5, 10, 20, 40 iterations.\n",
    "- Reset the noise after every episode.\n",
    "- Tried different suggested values for buffer size, batch size, gamma, tau, weight decay and the sigma variable in the Ornstein-Uhlenbeck process noise implementation.\n",
    "\n",
    "Even with all of the above modifications I still wasn't seeing any decent results. The score rarely went above 0.5, and seemed to get worse over time.\n",
    "\n",
    "### Structured Hyper-parameter Optimization\n",
    "\n",
    "After seeing a number of people say the suggested changes should work, but having no success myself, I felt that I needed to take a more robust approach to hyper-parameter optimization. Similar to my implementation in Project 1, I used scikit-optimize's Gaussian Process Optimizer to search for a better set of hyper-parameters. Below is the search space that I initially defined.\n",
    "\n",
    "I initially did batches of 10 episodes to keep the run time short and look for some better initial values. Because the process takes a long time I watched the values from each run, and visually tried to understand what values of each parameter were working well. As I noticed good values, I iteratively removed parameters from the search space, instead setting them at reasonable levels until I found something that worked fairly well which I've called Reasonable Parameter Set below.\n",
    "\n",
    "I have included the call to the optimization routine below, but haven't run it as it takes up a lot of space, and I re-ran it iteratively so one single output was never the final one used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "space = [\n",
    "    Real(0, 0.5, \"uniform\", name='eps_end'),\n",
    "    Real(1e-5, 1e0, \"log-uniform\", name='eps_decay'),\n",
    "    Categorical([1e5, 1e6], name=\"buffer_size\"),\n",
    "    Categorical([64, 128, 256, 512], name=\"batch_size\"),\n",
    "    Categorical([1, 5, 10, 20, 40], name=\"update_every\"),\n",
    "    Categorical([1, 5, 10, 20], name=\"update_times\"),\n",
    "    Categorical([128, 256, 512], name=\"actor_fc1_units\"),\n",
    "    Categorical([128, 256, 512], name=\"actor_fc2_units\"),\n",
    "    Categorical([128, 256, 512], name=\"critic_fc1_units\"),\n",
    "    Categorical([128, 256, 512], name=\"critic_fc2_units\"),\n",
    "    Real(1e-5, 1e-3, \"log-uniform\", name='actor_lr'),\n",
    "    Real(1e-5, 1e-3, \"log-uniform\", name='critic_lr'),\n",
    "    Real(0.9, 0.99, \"uniform\", name=\"gamma\"),\n",
    "    Real(1e-2, 1e-1, \"log-uniform\", name=\"tau\"),\n",
    "    Real(0, 1e-3, \"uniform\", name=\"weight_decay\"),\n",
    "    Real(0.01, 0.05, \"log-uniform\", name=\"noise_theta\"),\n",
    "    Real(0.01, 0.05, \"log-uniform\", name=\"noise_sigma\"),\n",
    "]\n",
    "\n",
    "params, res_gp = find_optimal_hyperparameters(env=env, brain_name=brain_name, num_agents=num_agents, n_calls=50, episodes_per_batch=10, space=space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasonable Parameter Set\n",
    "\n",
    "This is the best resonable parameter set I found through a somewhat manual search for hyper-parameters. There are many parameters to tune here, and the search space grows so quickly that a full optimization run was going to take more time than I had available, so I viewed the results and manually tuned based on what I saw to develop the set of values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"random_seed\": 22,\n",
      "  \"noise_sigma\": 0.02,\n",
      "  \"noise_theta\": 0.015,\n",
      "  \"weight_decay\": 0,\n",
      "  \"tau\": 0.05,\n",
      "  \"gamma\": 0.95,\n",
      "  \"critic_lr\": 0.0001,\n",
      "  \"actor_lr\": 0.001,\n",
      "  \"critic_fc2_units\": 128,\n",
      "  \"critic_fc1_units\": 128,\n",
      "  \"actor_fc2_units\": 128,\n",
      "  \"actor_fc1_units\": 128,\n",
      "  \"update_times\": 10,\n",
      "  \"update_every\": 20,\n",
      "  \"batch_size\": 256,\n",
      "  \"buffer_size\": 1000000.0,\n",
      "  \"eps_decay\": 0.7,\n",
      "  \"eps_end\": 0.3,\n",
      "  \"eps_start\": 1.0,\n",
      "  \"action_size\": 4,\n",
      "  \"state_size\": 33,\n",
      "  \"n_episodes\": 2000,\n",
      "  \"solved_threshold\": 31.0,\n",
      "  \"break_early\": true,\n",
      "  \"name\": \"_optimal\",\n",
      "  \"num_agents\": 1,\n",
      "  \"brain_name\": \"ReacherBrain\"\n",
      "Replay Buffer Size: 1000000.0, Batch Size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitson_swann/Documents/udacity_drl/udacity_drl_p2/src/ddpg_agent.py:140: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.69\n",
      "Episode 200\tAverage Score: 7.48\n",
      "Episode 300\tAverage Score: 13.16\n",
      "Episode 400\tAverage Score: 16.45\n",
      "Episode 500\tAverage Score: 23.23\n",
      "Episode 600\tAverage Score: 29.82\n",
      "Episode 639\tAverage Score: 31.07\n",
      "Environment solved in 639 episodes!\tAverage Score: 31.07\n"
     ]
    }
   ],
   "source": [
    "optimal_params = {\n",
    "  \"random_seed\": 22,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 128,\n",
    "  \"critic_fc1_units\": 128,\n",
    "  \"actor_fc2_units\": 128,\n",
    "  \"actor_fc1_units\": 128,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": 4,\n",
    "  \"state_size\": 33,\n",
    "  \"n_episodes\": 2000,\n",
    "  \"solved_threshold\": 31.0,\n",
    "  \"break_early\": True,\n",
    "  \"name\": \"_optimal\",\n",
    "  \"num_agents\": 1,\n",
    "  \"brain_name\": \"ReacherBrain\"\n",
    "}\n",
    "\n",
    "scores = train_ddpg(env=env, **optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"scores.json\", \"w\") as f:\n",
    "    json.dump({\"scores\": scores}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success !!!\n",
    "\n",
    "As you can see above the environment was solved in 639 episodes. In reality, it was a bit shorter than that, because I set the threshold to 31 just to go a bit beyond the minimim value.\n",
    "\n",
    "Below you can see the plots of raw scores and average scores that I took from Tensorboard.\n",
    "\n",
    "![tensorboard_raw_score.png](tensorboard_raw_score.png)\n",
    "![tensorboard_avg_score.png](tensorboard_avg_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch The Trained Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've loaded the trained agent and shown a gif of it performing the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer Size: 1000000.0, Batch Size: 256\n",
      "Timestep: 1001, Reward: 0.04, Mean: 0.0358\r"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='./env/v1/Reacher.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "from ddpg_agent import Agent\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "  \"random_seed\": 2,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 128,\n",
    "  \"critic_fc1_units\": 128,\n",
    "  \"actor_fc2_units\": 128,\n",
    "  \"actor_fc1_units\": 128,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": 4,\n",
    "  \"state_size\": 33\n",
    "}\n",
    "\n",
    "agent = Agent(**params)\n",
    "\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor_optimal.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic_optimal.pth'))\n",
    "        \n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "agent.reset()\n",
    "states = env_info.vector_observations\n",
    "agent_scores = []\n",
    "i = 1\n",
    "while True:\n",
    "    print(f\"Timestep: {i}\", end = \"\\r\")\n",
    "    actions = agent.act(state=states, add_noise=True)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    states = next_states\n",
    "    agent_scores.append(rewards[0])\n",
    "    print(f\"Timestep: {i}, Reward: {round(rewards[0],4)}, Mean: {round(np.mean(agent_scores),4)}\", end = \"\\r\")\n",
    "    agent_scores\n",
    "    if np.any(dones):\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![playing.gif](playing.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Training Run with Larger Network\n",
    "\n",
    "I wanted to see how much better the score could get with a slightly longer training run and networks with larger hidden layers. I also modified the batch size since it seems to have an effect on how long each iteration takes.\n",
    "\n",
    "Below you can see that performance improved slightly breaking the previous score, but it seemed to be hitting a plateau just above 30, so I terminated it.\n",
    "\n",
    "After understanding that there are always 1000 time steps in an episode, and the rewards are closer to 0.04 rather than 0.10 as per timestep that the arm is touching the goal, I realized there is an upper limit on the score of 0.40 * 1000 = 40. I think this is why the agent plateues just after a score of 30, because it is getting closer and closer to a perfect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"random_seed\": 22,\n",
      "  \"noise_sigma\": 0.02,\n",
      "  \"noise_theta\": 0.015,\n",
      "  \"weight_decay\": 0,\n",
      "  \"tau\": 0.05,\n",
      "  \"gamma\": 0.95,\n",
      "  \"critic_lr\": 0.0001,\n",
      "  \"actor_lr\": 0.001,\n",
      "  \"critic_fc2_units\": 300,\n",
      "  \"critic_fc1_units\": 400,\n",
      "  \"actor_fc2_units\": 300,\n",
      "  \"actor_fc1_units\": 400,\n",
      "  \"update_times\": 10,\n",
      "  \"update_every\": 20,\n",
      "  \"batch_size\": 256,\n",
      "  \"buffer_size\": 500000.0,\n",
      "  \"eps_decay\": 0.7,\n",
      "  \"eps_end\": 0.3,\n",
      "  \"eps_start\": 1.0,\n",
      "  \"action_size\": 4,\n",
      "  \"state_size\": 33,\n",
      "  \"n_episodes\": 5000,\n",
      "  \"solved_threshold\": 31.0,\n",
      "  \"break_early\": false,\n",
      "  \"name\": \"_alternate\",\n",
      "  \"num_agents\": 1,\n",
      "  \"brain_name\": \"ReacherBrain\"\n",
      "Replay Buffer Size: 500000.0, Batch Size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitson_swann/Documents/udacity_drl/udacity_drl_p2/src/ddpg_agent.py:140: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 2.96\n",
      "Episode 200\tAverage Score: 14.06\n",
      "Episode 300\tAverage Score: 27.03\n",
      "Episode 343\tAverage Score: 31.07\n",
      "Environment solved in 343 episodes!\tAverage Score: 31.07\n",
      "Episode 344\tAverage Score: 31.02\n",
      "Environment solved in 344 episodes!\tAverage Score: 31.02\n",
      "Episode 345\tAverage Score: 31.15\n",
      "Environment solved in 345 episodes!\tAverage Score: 31.15\n",
      "Episode 346\tAverage Score: 31.20\n",
      "Environment solved in 346 episodes!\tAverage Score: 31.2\n",
      "Episode 347\tAverage Score: 31.20\n",
      "Environment solved in 347 episodes!\tAverage Score: 31.2\n",
      "Episode 348\tAverage Score: 31.24\n",
      "Environment solved in 348 episodes!\tAverage Score: 31.24\n",
      "Episode 349\tAverage Score: 31.31\n",
      "Environment solved in 349 episodes!\tAverage Score: 31.31\n",
      "Episode 350\tAverage Score: 31.29\n",
      "Environment solved in 350 episodes!\tAverage Score: 31.29\n",
      "Episode 351\tAverage Score: 31.34\n",
      "Environment solved in 351 episodes!\tAverage Score: 31.34\n",
      "Episode 352\tAverage Score: 31.43\n",
      "Environment solved in 352 episodes!\tAverage Score: 31.43\n",
      "Episode 353\tAverage Score: 31.51\n",
      "Environment solved in 353 episodes!\tAverage Score: 31.51\n",
      "Episode 354\tAverage Score: 31.56\n",
      "Environment solved in 354 episodes!\tAverage Score: 31.56\n",
      "Episode 355\tAverage Score: 31.55\n",
      "Environment solved in 355 episodes!\tAverage Score: 31.55\n",
      "Episode 356\tAverage Score: 31.62\n",
      "Environment solved in 356 episodes!\tAverage Score: 31.62\n",
      "Episode 357\tAverage Score: 31.66\n",
      "Environment solved in 357 episodes!\tAverage Score: 31.66\n",
      "Episode 358\tAverage Score: 31.67\n",
      "Environment solved in 358 episodes!\tAverage Score: 31.67\n",
      "Episode 359\tAverage Score: 31.68\n",
      "Environment solved in 359 episodes!\tAverage Score: 31.68\n",
      "Episode 360\tAverage Score: 31.79\n",
      "Environment solved in 360 episodes!\tAverage Score: 31.79\n",
      "Episode 361\tAverage Score: 31.79\n",
      "Environment solved in 361 episodes!\tAverage Score: 31.79\n",
      "Episode 362\tAverage Score: 31.88\n",
      "Environment solved in 362 episodes!\tAverage Score: 31.88\n",
      "Episode 363\tAverage Score: 31.97\n",
      "Environment solved in 363 episodes!\tAverage Score: 31.97\n",
      "Episode 364\tAverage Score: 31.95\n",
      "Environment solved in 364 episodes!\tAverage Score: 31.95\n",
      "Episode 365\tAverage Score: 32.05\n",
      "Environment solved in 365 episodes!\tAverage Score: 32.05\n",
      "Episode 366\tAverage Score: 32.07\n",
      "Environment solved in 366 episodes!\tAverage Score: 32.07\n",
      "Episode 367\tAverage Score: 32.06\n",
      "Environment solved in 367 episodes!\tAverage Score: 32.06\n",
      "Episode 368\tAverage Score: 32.18\n",
      "Environment solved in 368 episodes!\tAverage Score: 32.18\n",
      "Episode 369\tAverage Score: 32.13\n",
      "Environment solved in 369 episodes!\tAverage Score: 32.13\n",
      "Episode 370\tAverage Score: 32.19\n",
      "Environment solved in 370 episodes!\tAverage Score: 32.19\n",
      "Episode 371\tAverage Score: 32.32\n",
      "Environment solved in 371 episodes!\tAverage Score: 32.32\n",
      "Episode 372\tAverage Score: 32.30\n",
      "Environment solved in 372 episodes!\tAverage Score: 32.3\n",
      "Episode 373\tAverage Score: 32.39\n",
      "Environment solved in 373 episodes!\tAverage Score: 32.39\n",
      "Episode 374\tAverage Score: 32.45\n",
      "Environment solved in 374 episodes!\tAverage Score: 32.45\n",
      "Episode 375\tAverage Score: 32.49\n",
      "Environment solved in 375 episodes!\tAverage Score: 32.49\n",
      "Episode 376\tAverage Score: 32.52\n",
      "Environment solved in 376 episodes!\tAverage Score: 32.52\n",
      "Episode 377\tAverage Score: 32.57\n",
      "Environment solved in 377 episodes!\tAverage Score: 32.57\n",
      "Episode 378\tAverage Score: 32.66\n",
      "Environment solved in 378 episodes!\tAverage Score: 32.66\n",
      "Episode 379\tAverage Score: 32.69\n",
      "Environment solved in 379 episodes!\tAverage Score: 32.69\n",
      "Episode 380\tAverage Score: 32.72\n",
      "Environment solved in 380 episodes!\tAverage Score: 32.72\n",
      "Episode 381\tAverage Score: 32.72\n",
      "Environment solved in 381 episodes!\tAverage Score: 32.72\n",
      "Episode 382\tAverage Score: 32.69\n",
      "Environment solved in 382 episodes!\tAverage Score: 32.69\n",
      "Episode 383\tAverage Score: 32.67\n",
      "Environment solved in 383 episodes!\tAverage Score: 32.67\n",
      "Episode 384\tAverage Score: 32.70\n",
      "Environment solved in 384 episodes!\tAverage Score: 32.7\n",
      "Episode 385\tAverage Score: 32.72\n",
      "Environment solved in 385 episodes!\tAverage Score: 32.72\n",
      "Episode 386\tAverage Score: 32.80\n",
      "Environment solved in 386 episodes!\tAverage Score: 32.8\n",
      "Episode 387\tAverage Score: 32.85\n",
      "Environment solved in 387 episodes!\tAverage Score: 32.85\n",
      "Episode 388\tAverage Score: 32.93\n",
      "Environment solved in 388 episodes!\tAverage Score: 32.93\n",
      "Episode 389\tAverage Score: 33.02\n",
      "Environment solved in 389 episodes!\tAverage Score: 33.02\n",
      "Episode 390\tAverage Score: 33.08\n",
      "Environment solved in 390 episodes!\tAverage Score: 33.08\n",
      "Episode 391\tAverage Score: 33.14\n",
      "Environment solved in 391 episodes!\tAverage Score: 33.14\n",
      "Episode 392\tAverage Score: 33.24\n",
      "Environment solved in 392 episodes!\tAverage Score: 33.24\n",
      "Episode 393\tAverage Score: 33.23\n",
      "Environment solved in 393 episodes!\tAverage Score: 33.23\n",
      "Episode 394\tAverage Score: 33.26\n",
      "Environment solved in 394 episodes!\tAverage Score: 33.26\n",
      "Episode 395\tAverage Score: 33.27\n",
      "Environment solved in 395 episodes!\tAverage Score: 33.27\n",
      "Episode 396\tAverage Score: 33.23\n",
      "Environment solved in 396 episodes!\tAverage Score: 33.23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-66924f3da8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m }\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0malternate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/udacity_drl/udacity_drl_p2/src/train.py\u001b[0m in \u001b[0;36mtrain_ddpg\u001b[0;34m(env, brain_name, num_agents, name, break_early, solved_threshold, n_episodes, state_size, action_size, eps_start, eps_end, eps_decay, buffer_size, batch_size, update_every, update_times, actor_fc1_units, actor_fc2_units, critic_fc1_units, critic_fc2_units, actor_lr, critic_lr, gamma, tau, weight_decay, noise_theta, noise_sigma, random_seed)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/udacity_drl/udacity_drl_p2/src/ddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/udacity_drl/udacity_drl_p2/src/ddpg_agent.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;34m\"\"\"Randomly sample a batch of experiences from memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/udacity_drl_p2/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mselected_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alternate_params = {\n",
    "  \"random_seed\": 22,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 300,\n",
    "  \"critic_fc1_units\": 400,\n",
    "  \"actor_fc2_units\": 300,\n",
    "  \"actor_fc1_units\": 400,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 500000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": 4,\n",
    "  \"state_size\": 33,\n",
    "  \"n_episodes\": 5000,\n",
    "  \"solved_threshold\": 31.0,\n",
    "  \"break_early\": False,\n",
    "  \"name\": \"_alternate\",\n",
    "  \"num_agents\": 1,\n",
    "  \"brain_name\": \"ReacherBrain\"\n",
    "}\n",
    "\n",
    "scores = train_ddpg(env=env, **alternate_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for Future Work\n",
    "\n",
    "If I had more time and access to a GPU I could conduct a more exhaustive hyper-parameter search, letting the agent run for at least 100 episodes each time and I would probably find a better set of parameters, but I don't think this would lead to a significantly better solution for the reason I mentioned above that there isan upper limit on performance. It may lead to a faster to train and more efficient solution though.\n",
    "\n",
    "Using the same algorithm, I could also make slight modifications to train with the multi-agent version of the Reacher environment. I did try this but didn't see initially better performance and it seemed that the hyper-parameters were more important. Now that I've found some decent options I could further investigate the multi-agent version.\n",
    "\n",
    "In the project instructions, there were also number of other algorithms mentioned that may work better including:\n",
    "\n",
    "- Trust Region Policy Optimization (TRPO)\n",
    "- Truncated Natural Policy Gradient (TNPG)\n",
    "- Proximal Policy Optimization (PPO)\n",
    "- Distributed Distributional Deterministic Policy Gradients (D4PG)\n",
    "\n",
    "The next steps for me would be to investigate and attempt to solve the task with one or more of these alternate algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
